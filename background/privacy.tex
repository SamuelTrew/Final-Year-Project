With various methods of ML being applied to all manner of things to try and help make better predictions, it is only natural that the data in question will include personal data. 
Whether this is through monitoring app/website usage or simply tracking what people send to each other, the data is personal to each user and as such the experience of each user is personal. 
\\ \\
However, data can stray beyond this into more truly personal information such as that of the medical nature.
This is trickier to deal with from an analytical perspective, given that medical data is very tightly controlled and restricted due to its highly sensitive nature and the governing bodies controlling it are usually very unwilling to share. 
Now in the past this was not really an issue as typically only medical professionals of the governing organisation (e.g. the National Health Service (NHS)) would want access to this data and actually have any good reason. 
So it was very easy and efficient to deny any external access. 
However, we now live in a world where the prevalence of ML has made external researchers' access to data increasingly valuable to society. 
Users, beyond medical professionals, want access to this highly sensitive data to try and develop new, beneficial tools.
\\ \\
Whether this is in trying to train a model to recognise cancerous cells or early onset dementia, data surrounding that is needed to accurately make predictions. 
So, to be able to push the field of medicine forward and to leverage to full power of modern day computing, access to medical data needs to be granted to software engineers.
So several methods of trying to preserve the privacy of the users of this data have been created to tackle this. Only methods directly relevant to our work will be covered in this project.


\subsection{K-Anonymisation}
The idea behind anonymising data is to not be able to link an individual person's data record with their actual identity in a given dataset. This gives reassurance to the individual, whose private data has been given to the system, that there will be no risk of association and its subsequent repercussions. This depersonalises data, so that is able to be used beyond the governing owner of that data. 
It is hugely beneficial to both parties and is a great starting point for preserving the privacy of the data. 
\\ \\
However, this is not as simple as it sounds and many attempts to pseudo-anonymise the data have resulted in people able to link medical data with more available (and less sensitive) data like a voter list \cite{failed_anonymisation}.
\\ \\
Even if that was not the case, there are certain attributes that when taken together are potentially able to identify a person. These are called quasi-identifiers. 
An example would be if you knew their post code and their date-of-birth, you could potentially figure out a disease they have if no one in that postcode shares that date-of-birth with them. e.g. id=3 in Table \ref{tbl:pre-anon}.
\\ \\
A way to tackle these is through the use of k-anonymising the data. A table of data is k-anonymous if every record in the table is indistinguishable from at least k-1 other records, with respect to every set of quasi-identifiers (not something that is itself a unique identifier, but when paired with other quasi-identifiers it can identify someone). 
The 2 styles of doing this are perturbative methods and non-perturbative methods but seeing as perturbative methods (e.g. adding noise) does not retain data integrity, we will not be considering it. 
Two main ways of achieving k-anonymity for non-perturbative methods are generalising the data (replacing attribute values with something more generalised e.g. 54321 $\longrightarrow$ 54***) and suppression (deletion of row or column e.g. row with id=3 in Table \ref{tbl:pre-anon}).
\begin{center}
    \begin{longtable}{ |c|c|c|c|c| }
    \caption{Pre-Anonymisation}
    \label{tbl:pre-anon}
    \hline
    \textbf{id} & \textbf{sex} & \textbf{DoB} & \textbf{postcode} & \textbf{disease} \ \\ \hline
    1 & female & 1998-03-13 & SW7 2BU & anxiety \ \\ \hline
    2 & female & 1998-10-27 & NW1 9LJ & anxiety \ \\ \hline
    3 & female & 1997-01-22 & AL9 7TA & chicken pox \ \\ \hline
    4 & male & 1999-11-11 & HP11 1UA & ADD \ \\ \hline
    5 & male & 1999-06-18 & HP14 4JQ & Dyspraxia \ \\ \hline
    6 & male & 1999-06-21 & HP11 2DQ & ADHD \ \\ \hline
    \end{longtable}
    
    \begin{longtable}{ |c|c|c|c|c| }
    \caption{2-Anonymised}
    \label{tbl:2-anon}
    \hline
    \textbf{id} & \textbf{sex} & \textbf{DoB} & \textbf{postcode} & \textbf{disease} \ \\ \hline
    \rowcolor{lightgray} 1 & female & 1998 & London & anxiety \ \\ \hline
    \rowcolor{lightgray} 2 & female & 1998 & London & anxiety \ \\ \hline
    \rowcolor{gray} 4 & male & 1999 & High Wycombe & ADD \ \\ \hline
    \rowcolor{gray} 5 & male & 1999 & High Wycombe & Dyspraxia \ \\ \hline
    \rowcolor{gray} 6 & male & 1999 & High Wycombe & ADHD \ \\ \hline
    \end{longtable}
\end{center}

The data is still susceptible to homogeneity and semantic attacks [Table \ref{tbl:2-anon}] and while {l-Diversity} can fix the former there is never any guarantee that a dataset is 100\% completely and truly anonymised. With k-anonymisation, a specific data record cannot be identified with any given individual but information can still be inferred. The governing body behind the data sometimes has to make a decision about what is an acceptable level of anonymisation without completely destroying any utility or functionality behind it. There are some techniques being investigated through clustering of the data to help minimise information loss \cite{anon_cluster} but again, these only go so far. It should also be noted that k-anonymity can only work on table and record style data, and will be limited in the case of images.


\subsection{Differential Privacy}
Another method for preserving privacy is Differential Privacy (DP). DP guarantees that when you query a dataset D, the result that you get will be approximately the same (if not the same) irrespective of the presence of whether or not the dataset contained a certain value. This can be summarised as:
\begin{equation}
    Pr[x | y \in D] \approx Pr[x | y \notin D]
\end{equation}
As we want = instead of $\approx$ without destroying utility, we have the difference shown with $\epsilon$ such that:
\begin{equation}
    Pr[M(D) = y] \leq e^\epsilon Pr[M(D') = y]
\end{equation}
Where D is the dataset with no changes and D' is the dataset with the missing row. M is the function applied to D that computes the output and ensures privacy. We have to make sure that the two datasets satisfy the condition that they are neighbouring, i.e. they differ by exactly one row. This is what it means for a mechanism to be $\epsilon$-Differentially private. \\ \\
For achieving DP, the key ingredient is adding noise to the data until the above requirement is satisfied. This can be done with either Gaussian or Laplacian noise, with Laplace noise being the more common choice for DP. Typically, you draw from a Laplacian distribution that is $L(1/\epsilon)$ for the mechanism to remain $\epsilon$-Differentially private. Laplace's probability density function (pdf) is defined as:
\begin{equation}
    L(x|\mu,b) = \dfrac{1}{2b} \exp \left( -\dfrac{|x - \mu|}{b}\right)
\end{equation}

For ML models, depending on where we add the noise to, our mechanism will either be Locally or Globally differentially private. If the noise is added to the inputs of the model then it is Local DP. This involves each client adding noise so as to help protect their data. This helps ensure that each user has plausible deniability as you could never accurately claim that a certain client's specific data is of some specific value. If the noise is added to the end then it is Global DP. This is more desirable if the owner of the person taking the data and training with it is trustworthy as you can normally achieve more accurate results \cite{global_dp} as the input data has not been modified or changed. \\ \\
Interestingly, there are cases where adding noise to the gradient of models in very deep NNs \cite{dnn_noise} or as a form of regularisation for data augmentation \cite{robust_corrupt_noise} can help improve the generalisation performance of the trained model through either greater feature extraction ability or from reducing over-fitting.