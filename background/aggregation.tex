Evidently, having a raw aggregation method is suspect to a variety of potential attacks from any number of malicious users. In some cases it might not take more than one client to have the model predict inaccurately. So, unfortunately for FedSGD and FedAvg, we have to look elsewhere for a more robust aggregation strategy. This can typically involve not including the malicious clients in the aggregation, giving the malicious clients a lower weighting in the aggregation and/or just flat-out blocking the clients once a certain threshold is met. \\ \\
Any of these methods have their cons in that it is possible in all cases to end up blocking at least one of the benign clients as well. 
When the number of clients is small this can have a much more detrimental affect due to a significant chunk of the training data is lost. 
Increasing the number of clients would mitigate the effect of this but there is still an issue of have the benign clients' data lost. 
A couple of different methods for robust aggregation will be introduced along with their performance under different styles of attacks.

\subsection{Krum}
To tolerate f byzantine clients (malicious/faulty) out of a total of n clients, Krum \cite{krum} achieves its byzantine-resilience property by looking at every subset of $n-f-1$ clients. 
It goes through each client and calculates the sum of the squared-distances between the client and their closest $n-f-2$ neighbours.
This becomes the score of that client and Krum takes the client with the best score and uses a data sample from that client.
With this method, Krum is able to theoretically tolerate up to f byzantine clients under the condition of $2f + 2 < n$. 
\\ \\
Another development on Krum brings in the idea of averaging to it, called Multi-Krum (MKRUM).
You take the top $n-f$ clients' parameter vectors and average them together so as to be able to better leverage multiple correct (or at least correct by KRUM standards) clients.
This helps to reduce waste and not to throw away data. However, MKRUM still (and even more so Krum) throws away a lot of clients' data that might be useful. It also takes on a position where $f$ might not be chosen correctly, leading to even more data unnecessarily being thrown away.

\subsection{Coordinate-wise Median}
A simpler approach to choosing an individual client's parameters is through Coordinate-wise Median (COMED) \cite{comed}.
For all parameter vectors from all of the clients, COMED takes the median value of each of the parameters throughout all of the models.
This is defined mathematically as
\begin{equation}
    g_k = med\{x_k^i: i \in [m]\}
\end{equation}
where k is the k-th parameter and m is the number of clients involved.
This very nicely gets to utilise all of the clients to try to have as balanced of a choice as possible for all of the model parameters.
It also performs quite well, generally better than MKRUM in an adversarial test while being a bit more sensitive to noise without any bad clients involved \cite{robagg_health}.

\subsection{Adaptive Federated Averaging}
A different way to handle byzantine clients is to simply block them and that is precisely what Adaptive Federated Averaging (AFA) \cite{afa} does.
It determines if a client is malicious based on the cosine similarity ($s_k$) between the model updates from the clients and the global aggregated model's parameters. \\ \\
Assuming that the number of byzantine clients is less than half of the total clients, we use the median ($\hat{\mu}_s$), mean ($\bar{\mu}_s$) and standard deviation ($\sigma_s$) of the similarities of the clients. AFA works as $\hat{\mu}_s$ must be one of the good values due to the the number of byzantine clients being less than half. We then make two comparisons for determining if a client is bad:
\begin{equation}
    \begin{cases}
      \hat{\mu}_s < \bar{\mu}_s \longrightarrow s_k < \bar{\mu}_s - \xi \sigma_s\\
      \hat{\mu}_s \geq \bar{\mu}_s \longrightarrow s_k < \bar{\mu}_s + \xi \sigma_s\\
    \end{cases}
\end{equation}

If either of these constraints is met, then the client is deemed to have sent a bad update and so is blocked.
This strategy for robust aggregation is also shown to consistently outperform previous methods on certain datasets \cite{robagg_health}.

\subsection{FedMGDA+}
Another method involving the blocking of clients is FedMGDA+ \cite{fedmgda}. 
This takes a subset of clients and for each one, performs a client update.
This involves a single layer NN that aims to minimise the loss and for each client, the new weights formed are returned as a difference to the global model weights.
The central server then aims to learn the relevance of each client based on the importance assigned to it. 
This importance is initially chosen based on dataset size of the respective client.
Malicious clients are hopefully deemed less important until they reach a certain threshold and are then subsequently blocked.
\\ \\
The relative performance of FedMGDA+ really shines through when comparing its performance with and without attacks \cite{fedmgda}.
For no attacks, it tends to perform a bit worse on average compared to other aggregation methods like FedAvg.
However, when attacks are introduced, it performs much better than certain other aggregators due to its dynamic weighting system in place.

\subsection{Group-wise Robust Aggregation}
A different approach is to try and minimise the variance of model parameters so that we can improve the resiliency of the aggregation.
One technique of doing this is through group-wise robust aggregation (GroupRA) \cite{cluster_robagg}.
Here the model parameters from the clients are clustered based on similarity (e.g. difference between parameter values).
A popular choice would be k-means clustering but this also has some extra overhead of trying to find the optimal k.
Through trial and error this should not be too hard though.
We can then apply robust aggregation to each cluster individually using any aggregation of choice (e.g. MKRUM, COMED).
\\ \\
Clustering gives byzantine clients 2 options, to be spread out or for them to try to stick together and actively deviate the centre of the cluster.
\begin{enumerate}
    \item Here the clusters can be made smaller, essentially relying on the robust aggregation to eliminate the byzantine clients
    
    \item Here the byzantine clusters can collectively try and make their cluster be moved towards their ideal target.
    This somewhat falls down due to the majority of clusters being benign.
    So when the cluster centres are used for the global model aggregation, the weighting of the scoring function helps minimise the deviation introduced.
\end{enumerate}
\\ \\
We then need to decide on a method of scoring.
A typical choice would be to have each cluster weighted by its normalised cluster size.
A more refined method would be to take the single cluster centre for each cluster and apply that to the global model and test it on a small dataset.
Weights are then assigned based on their accuracy but it could be quite a computationally expensive process.
\\ \\
When tested under a model poisoning attack with CIFAR-10 and MNIST, GroupRA outperformed all other robust aggregation strategies tested (e.g. KRUM, Median). 
It should be noted though that there were some k-means clustering optimisation going on here as well.
Unoptimised k values end up performing just as badly as the other aggregation methods.
This is because with too few or too many clusters, you essentially just get the standard robust aggregation method used.

\subsection{Spectral Anomaly Detection \& Variational Auto-encoder}
Spectral Anomaly Detection (SAD) is a highly effective anomaly detection method \cite{sad}.
The data is embedded into a low-dimensional state regardless of if it's benign or not.
Through looking at reconstruction errors and learning to remove noisy features, you should easily be able to identify abnormal instances \cite{variationalAB}.
\\ \\
Through the use of an encoder-decoder architecture \cite{spectral}, you can train it to recognise byzantine clients as they trigger a higher reconstruction error.
The encoder takes the original data and does the SAD to output the low-dimensional embeddings.
The decoder then creates a reconstruction error from these embeddings and uses that error to optimise the parameters of the encoder-decoder model until convergence.
\\ \\
For the encoder, a Variational Auto-Encoder (VAE) is used with dynamic thresholding.
This has benefits over other methods as the detection threshold is only created after the central server has received all of the clients' model updates.
This stops the byzantine clients from knowing the detection mechanism beforehand, which can help with them not being able to carry out more clever model-poisoning attacks to escape being caught.
\\ \\
The theory behind why this works is that the byzantine clients have to make quite significant changes for their model update to have any real effect.
This is due to the averaging nature of the base aggregation of FedAvg or FedSGD.
So these larger updates become more noticeable under the scope of the encoder-decoder model due to the size of the reconstruction error.
\\ \\
The ways in which other aggregation methods can fail where SAD VAE doesn't are:
\begin{enumerate}
    \item Hard-Thresholding (Sign-Flipping): the VAE uses dynamic thresholding at each round after receiving the model updates and so isn't susceptible.
    
    \item Noise: adding some noise to the model can help protect privacy through DP but when too much is added it can hurt the model's accuracy. 
    The SAD helps negate this somewhat by having the step to reduce noisy features so as to focus on the more import features of the data.
    
    \item Model Poisoning: this requires prior knowledge of the system and seeing as the VAE is dynamic, trying to use crafted smaller updates to not get picked up won't work as well here.
\end{enumerate}
Through experiments done \cite{spectral}, it has been show that the model accuracy with attacks can come closer or be the same as model accuracy without attacks.


















%mention https://arxiv.org/pdf/1911.00222.pdf as in DP before rob agg